{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -i -t -v /data/data1/:/data -v /home:/hm hub.ai.xm.gov.cn/comp_120/geopandas /bin/bash \n",
    "clear\n",
    "python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/data/'\n",
    "DATA_TAXI_ORDER = DATA_PATH+'taxiOrder/'\n",
    "DATA_FILE_NAME = 'taxiOrder201951-54.csv'\n",
    "# DATA_FILE_NAME = 'taxiOrder_2018429-501.csv'\n",
    "DATA_FILE_OUTPUT_NAME = '/xm/gdf2018429-501.csv'\n",
    "# DATA_FILE_OUTPUT_NAME = '/xm/gdf201951-54.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_TAXI_ORDER+DATA_FILE_NAME)\n",
    "total_num = len(df)\n",
    "\n",
    "df.dropna(subset=['GETON_LONGITUDE', \n",
    "                  'GETON_LATITUDE', \n",
    "                  'GETOFF_LONGITUDE', \n",
    "                  'GETOFF_LATITUDE',\n",
    "                  'PASS_MILE',\n",
    "                  'CAR_NO'], inplace=True)\n",
    "na_num = len(df)\n",
    "\n",
    "df['GETON_DATE'] = pd.to_datetime(df['GETON_DATE'])\n",
    "df['GETOFF_DATE'] = pd.to_datetime(df['GETOFF_DATE'])\n",
    "\n",
    "df['trip_duration'] = (df['GETOFF_DATE'] - df['GETON_DATE']).dt.total_seconds()/60\n",
    "df = df[df['trip_duration'] > 1]\n",
    "more_then_1_num = len(df)\n",
    "\n",
    "# filter data \n",
    "lat_range = [24.2106, 25.0881]\n",
    "long_range = [117.3944, 118.8913]\n",
    "df_filtered = df[\n",
    "    (df['GETON_LONGITUDE'] > long_range[0])\n",
    "    & (df['GETON_LONGITUDE'] < long_range[1])\n",
    "    & (df['GETON_LATITUDE'] > lat_range[0])\n",
    "    & (df['GETON_LATITUDE'] < lat_range[1])\n",
    "    & (df['GETOFF_LONGITUDE'] > long_range[0])\n",
    "    & (df['GETOFF_LONGITUDE'] < long_range[1])\n",
    "    & (df['GETOFF_LATITUDE'] > lat_range[0])\n",
    "    & (df['GETOFF_LATITUDE'] < lat_range[1])\n",
    "    & (df['trip_duration'] > 0.5)\n",
    "    & (df['trip_duration'] < 37) # 34\n",
    "]\n",
    "in_range_num = len(df)\n",
    "\n",
    "df_filtered['day'] = df_filtered['GETON_DATE'].apply(\n",
    "    lambda dt: datetime.datetime(dt.year, dt.month, dt.day)\n",
    ")\n",
    "\n",
    "df_filtered['hour'] = df_filtered['GETON_DATE'].apply(\n",
    "    lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour)\n",
    ")\n",
    "# df_filtered['off_hour'] = df_filtered['GETOFF_DATE'].dt.hour\n",
    "\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df_filtered, geometry=gpd.points_from_xy(df_filtered['GETON_LATITUDE'],df_filtered['GETON_LONGITUDE'])\n",
    ")\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "df_geometry = [Point(xy) for xy in zip(df_filtered.GETON_LONGITUDE, df_filtered.GETON_LATITUDE)]\n",
    "gdf = gpd.GeoDataFrame(df_filtered, geometry=df_geometry)\n",
    "gdf.crs={'init':'epsg:4326'}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pytz   \n",
    "import math\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point,Polygon\n",
    "from fiona.crs import from_epsg\n",
    "\n",
    "\n",
    "lat_range = [24.2106, 25.0881]\n",
    "long_range = [117.3944, 118.8913]\n",
    "\n",
    "y = np.linspace(lat_range[0],lat_range[1],18)\n",
    "x = np.linspace(long_range[0],long_range[1],18)\n",
    "\n",
    "grid = []\n",
    "for m,i in enumerate(x[:17]):\n",
    "    for k,j in enumerate(y[:17]):\n",
    "        cord = np.array([i,j,x[m+1],j,x[m+1],y[k+1],i,y[k+1]]).reshape(4,2)\n",
    "        grid.append(Polygon(cord))\n",
    "\n",
    "\n",
    "area = gpd.GeoDataFrame(grid)\n",
    "area = area.reset_index()\n",
    "area.columns=['index','geometry']\n",
    "area.crs={'init':'epsg:4326'}\n",
    "\n",
    "\n",
    "\n",
    "t_gdf = gpd.sjoin(gdf, area, op='within', how='left')\n",
    "t_gdf.to_csv(DATA_FILE_OUTPUT_NAME, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pytz   \n",
    "import math\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point,Polygon\n",
    "from fiona.crs import from_epsg\n",
    "\n",
    "t_gdf = pd.read_csv(DATA_FILE_OUTPUT_NAME)\n",
    "t_gdf['hour'] = pd.to_datetime(t_gdf['hour'])\n",
    "\n",
    "\n",
    "lat_range = [24.2106, 25.0881]\n",
    "long_range = [117.3944, 118.8913]\n",
    "\n",
    "y = np.linspace(lat_range[0],lat_range[1],18)\n",
    "x = np.linspace(long_range[0],long_range[1],18)\n",
    "\n",
    "grid = []\n",
    "for m,i in enumerate(x[:17]):\n",
    "    for k,j in enumerate(y[:17]):\n",
    "        cord = np.array([i,j,x[m+1],j,x[m+1],y[k+1],i,y[k+1]]).reshape(4,2)\n",
    "        grid.append(Polygon(cord))\n",
    "\n",
    "\n",
    "area = gpd.GeoDataFrame(grid)\n",
    "area = area.reset_index()\n",
    "area.columns=['index','geometry']\n",
    "area.crs={'init':'epsg:4326'}\n",
    "\n",
    "t_stat = area.loc[:,['index']]\n",
    "\n",
    "l_l_c = []\n",
    "for key, item in t_gdf.groupby('index'):\n",
    "    t_dict = {}\n",
    "    for k, it in item.groupby('hour'):\n",
    "        t_dict[int(k.strftime('%s'))] = len(it)\n",
    "    l_l_c.append([int(key)]+[t_dict[i] for i in sorted(t_dict, key=t_dict.get)])\n",
    "\n",
    "\n",
    "t_df = pd.DataFrame(l_l_c, columns=['index']+[x for x in range(0,24*3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_l_c = []\n",
    "for key, item in t_gdf.groupby('index'):\n",
    "    t_dict = {}\n",
    "    for k, it in item.groupby('hour'):\n",
    "        t_dict[k.dt.total_seconds()] = len(it)\n",
    "    l_l_c.append([t_dict[i] for i in sorted(t_dict, key=t_dict.get)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_stat = pd.merge(t_stat,pd.DataFrame(l_l_c))\n",
    "\n",
    "t_stat = pd.merge(t_stat, t_gdf.groupby('index')['hour'].count().to_frame().reset_index(),\n",
    "               on='index', how='outer')\n",
    "t_stat.columns = ['index']+[i for i in range(0,24)]\n",
    "t_stat.fillna(0,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "series = gpd.read_file(DATA_FILE_OUTPUT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = series.values\n",
    "size = int(len(X)-24)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset into training and test \n",
    "train = all_valuse[:len(df_train), :]\n",
    "test = all_valuse[len(df_train):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train['Day']==\"2018-04-29 00:00:00\", 'Day'] = 1\n",
    "df_train.loc[df_train['Day']==\"2018-04-30 00:00:00\", 'Day'] = 2\n",
    "df_train.insert(0, 'count', df_train.pop('count'))\n",
    "\n",
    "df_test.loc[df_test['Day']==\"2018-05-01 00:00:00\", 'Day'] = 3\n",
    "df_test.insert(0, 'count', df_test.pop('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and outputs for model\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# reshape input to samples, time and features\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='rmse', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation\n",
    "params = {\n",
    "    # 参数设计.\n",
    "    'max_depth': 15\n",
    "    'eta':.03, \n",
    "    'subsample': 1, \n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective':'reg:linear',\n",
    "    'eval_metric':'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "CV=False\n",
    "if CV:\n",
    "    dtrain = xgb.DMatrix(train,label=y)\n",
    "    gridsearch_params = [\n",
    "        (eta)\n",
    "        for eta in np.arange(.04, 0.12, .02)\n",
    "    ]\n",
    "\n",
    "    # Define initial best params and RMSE\n",
    "    min_rmse = float(\"Inf\")\n",
    "    best_params = None\n",
    "    for (eta) in gridsearch_params:\n",
    "        print(\"CV with eta={} \".format(\n",
    "                                 eta))\n",
    "\n",
    "        # Update our parameters\n",
    "        params['eta'] = eta\n",
    "\n",
    "        # Run CV\n",
    "        cv_results = xgb.cv(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=1000,\n",
    "            nfold=3,\n",
    "            metrics={'rmse'},\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "\n",
    "        # Update best RMSE\n",
    "        mean_rmse = cv_results['test-rmse-mean'].min()\n",
    "        boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "        print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n",
    "        if mean_rmse < min_rmse:\n",
    "            min_rmse = mean_rmse\n",
    "            best_params = (eta)\n",
    "\n",
    "    print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))\n",
    "else:\n",
    "    #Print final params to use for the model\n",
    "    params['silent'] = 0 #Turn on output\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBoost模型\n",
    "def XGBmodel(x_train,x_test,y_train,y_test,params):\n",
    "    matrix_train = xgb.DMatrix(x_train,label=y_train)\n",
    "    matrix_test = xgb.DMatrix(x_test,label=y_test)\n",
    "    model=xgb.train(params=params,\n",
    "                    dtrain=matrix_train,num_boost_round=2000, \n",
    "                    early_stopping_rounds=100,evals=[(matrix_test,'test')])\n",
    "    return model\n",
    "\n",
    "model = XGBmodel(x_train,x_test,y_train,y_test,params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
